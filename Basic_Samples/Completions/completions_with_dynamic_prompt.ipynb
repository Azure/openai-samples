{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278e7451",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h1 align =\"center\"> Dynamic Prompting for task completion</h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805163f0-e5fe-4753-9e38-dc0f4c85bd2c",
   "metadata": {},
   "source": [
    "Recent papers such as [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/abs/2109.01247) and [What Makes Good In-Context Examples for GPT-3?](https://aclanthology.org/2022.deelio-1.10.pdf) have shown that using dynamic set of examples instead of fixed set of examples help GPT-3 to perfom the task with higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5655453",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if needed, upgrade to the latest version of the OpenAI Python library\n",
    "%pip install --upgrade openai\n",
    "%pip install --upgrade torch\n",
    "%pip install --upgrade sentence_transformers\n",
    "%pip install --upgrade numpy\n",
    "%pip install --upgrade datasets\n",
    "%pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbb9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os module & the OpenAI Python library for calling the OpenAI API\n",
    "# please make sure you have installed required libraries via pip install -r requirements.txt\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07658950-92ca-4a70-a665-a4e21c9ddf06",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5966957b-b507-4f37-bde9-24fdebada03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset from Huggingface's dataset library\n",
    "dataset = load_dataset(\"trec\")\n",
    "# name of the text and label column\n",
    "label_type = 'coarse_label'\n",
    "text_key = \"text\"\n",
    "# create mapping of ids2class and class2id\n",
    "id2class = dict((i, label) for i, label in enumerate(dataset['train'].features[label_type].names))\n",
    "class2id = dict((label, i) for i, label in enumerate(dataset['train'].features[label_type].names))\n",
    "# create a dictionary with classes as key and containing all the training examples within that class\n",
    "class2TrainDataset = dict((label, []) for label in dataset['train'].features[label_type].names)\n",
    "for example in dataset['train']:\n",
    "    label = id2class[example[label_type]]\n",
    "    class2TrainDataset[label].append(example[text_key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4ea26-5071-41d8-a142-3426ada87f58",
   "metadata": {},
   "source": [
    "# Task Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0442beee-70fb-4555-9e11-654fe0fa9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a prompt for asking LLM to perform a task\n",
    "task_prompt = \"As a Question Answering agent, your goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answering process.\\nFollowing are the semantic classes: [\"\n",
    "task_prompt += \", \".join([label for label in class2TrainDataset]) + \"]\"\n",
    "# a prompt for asking LLM to generate the output for current task\n",
    "query_prompt = \"\\nClassify the following question into one of the above classes.\\nquestion: \"\n",
    "answer_prompt = \"\\noutput: \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c2541-4a80-4dc5-b773-bc85452aad7e",
   "metadata": {},
   "source": [
    "# Setup OpenAI APIAAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a92251-1fab-46dc-9e98-29b7fb51a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config values\n",
    "with open(r'config.json') as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "    \n",
    "# Setting up the deployment name\n",
    "model_name = config_details['COMPLETIONS_MODEL']\n",
    "\n",
    "# This is set to `azure`\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_base = config_details['OPENAI_API_BASE']\n",
    "\n",
    "# Currently OPENAI API have the following versions available: 2022-12-01\n",
    "openai.api_version = config_details['OPENAI_API_VERSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03c4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text completion using GPT\n",
    "def trim_text(text):\n",
    "    return text.strip().strip('\\n').strip('\\\\n')\n",
    "    \n",
    "def generate_using_gpt(prompt):\n",
    "    generated_sentence = \"\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=model_name,\n",
    "        prompt=prompt, \n",
    "        max_tokens=3,\n",
    "        temperature=0,\n",
    "        top_p=1,\n",
    "        stop=None,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0.0)\n",
    "    choices = response.get(\"choices\", \"\")\n",
    "    if len(choices) == 0 or \"text\" not in choices[0]:\n",
    "        generated_sentence\n",
    "    generated_sentence = choices[0][\"text\"].lstrip('\\\\n').rstrip('\\\\n').lstrip('\\n\\n').rstrip('\\n\\n').lstrip('\\n').rstrip('\\n')\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d2126-c1b8-4942-ae60-e70f93265a3d",
   "metadata": {},
   "source": [
    "# Zero-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee4a12e-3ce0-457a-ab23-c94c42071b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt without any examples from the training dataset\n",
    "labels = []\n",
    "predictions = []\n",
    "for example in dataset['test']:\n",
    "    zeroshot_prompt = task_prompt +  query_prompt + example[text_key] + answer_prompt\n",
    "    pred = generate_using_gpt(zeroshot_prompt)\n",
    "    pred=trim_text(pred)\n",
    "    labels.append(example[label_type])\n",
    "    predictions.append(class2id[pred])\n",
    "        \n",
    "report = classification_report(labels, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aba42eb-7f20-45ea-a893-38cde49f6859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82         9\n",
      "           1       0.28      0.68      0.40        94\n",
      "           2       0.77      0.14      0.24       138\n",
      "           3       0.82      0.22      0.34        65\n",
      "           4       0.66      0.90      0.76        81\n",
      "           5       0.81      0.78      0.80       113\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.67      0.62      0.56       500\n",
      "weighted avg       0.68      0.54      0.51       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258912ca-9593-4fe0-9627-44dfcd93753a",
   "metadata": {},
   "source": [
    "# Few-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6269709e-a4e9-4a33-a4ff-96c6edc38204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to selection few examples in each of the classes from the training dataset\n",
    "def generateFewshotPrompt(class2TrainDataset, N=3):\n",
    "    fewshot_prompt = \"\\nFollowing are some examples.\"\n",
    "    for label in class2TrainDataset:\n",
    "        for example in class2TrainDataset[label][:N]:\n",
    "            fewshot_prompt += \"\\nquestion: \" + example\n",
    "            fewshot_prompt += \"\\noutput: \" + label\n",
    "    return fewshot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38f65537-04e7-4dee-a40d-a28973e27dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt is created by adding one example in each of the classes \n",
    "labels = []\n",
    "predictions = []\n",
    "fewshot_examples = generateFewshotPrompt(class2TrainDataset, N=1)\n",
    "for example in dataset['test']:\n",
    "    fewshot_prompt = task_prompt + fewshot_examples + query_prompt + example[text_key] + answer_prompt\n",
    "    pred = generate_using_gpt(fewshot_prompt)\n",
    "    pred=trim_text(pred)\n",
    "    labels.append(example[label_type])\n",
    "    predictions.append(class2id[pred])\n",
    "        \n",
    "report = classification_report(labels, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "724a5041-ded3-4fe9-9db9-d47556b6ba10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         9\n",
      "           1       0.39      0.74      0.51        94\n",
      "           2       0.87      0.42      0.57       138\n",
      "           3       0.91      0.45      0.60        65\n",
      "           4       0.95      0.88      0.91        81\n",
      "           5       0.84      1.00      0.91       113\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.78      0.75      0.73       500\n",
      "weighted avg       0.79      0.70      0.70       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1d828-72fa-4540-8995-157cce2c82c8",
   "metadata": {},
   "source": [
    "# Extract Embeddings for Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf81306b-1e58-49a1-913b-2569328d609c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac34aaa998d34612912af7ecb6672458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4230ae76547e4b14b7501574cd908f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338d8a8608d745098606a184f168556b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578295e54d334d38b873a896892ce79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31161c384a643fba216c0971b9db97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67260867218649aca57d395b4b6e8739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2722272d071a4ef396d473d6f5aa6488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734309c50c05450ebb12960474dd8ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467bdbf449354452a6f4dd8aab7ada84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a690170f373c48008684def231b9e9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65358bc84dba4f7fb012e93dcca9f271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4cda7c507b434991df9d9f3a50fa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6cf4d95307450ea11c5df1286ab945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8947423a9a94ae090590119caa1765d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading Sentence Transformer based model\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "\n",
    "# extract embeddings for a set of examples\n",
    "def ExtractEmbeddings(examples):\n",
    "    embedding_ls = []\n",
    "    for example in examples:\n",
    "        embedding = model.encode(example)     \n",
    "        embedding_ls.append(embedding)\n",
    "    return embedding_ls\n",
    "\n",
    "# extract embeddings for all the training examples\n",
    "class2TrainDatasetWithEmbedding = {}\n",
    "for label in class2TrainDataset:\n",
    "    embeddings = ExtractEmbeddings(class2TrainDataset[label])\n",
    "    class2TrainDatasetWithEmbedding[label] = [class2TrainDataset[label], embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c8ce6-60a0-4abc-8d00-5cbc82ba6c00",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dynamic Few-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42cb4a5-fa35-441e-96ec-76edcd5ae4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract similar queries for a given input text from each of the classes\n",
    "def getSimilarExamples(input_text, dataset, dataset_embedding):\n",
    "    input_embedding = model.encode(input_text)\n",
    "    sim_score = util.dot_score(input_embedding, dataset_embedding)[0]\n",
    "    topN_ids = np.argsort(-sim_score)\n",
    "    return [dataset[i] for i in topN_ids]\n",
    "    \n",
    "def getClasswiseSimilarExamples(input_text, class2TrainDatasetWithEmbedding):\n",
    "    classwiseSimilarExamples = {}\n",
    "    for label in class2TrainDataset:\n",
    "        similarExamples = getSimilarExamples(input_text, class2TrainDatasetWithEmbedding[label][0], class2TrainDatasetWithEmbedding[label][1])\n",
    "        classwiseSimilarExamples[label] = similarExamples\n",
    "    return classwiseSimilarExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78d83dfc-fa32-4d86-a730-6a63ff3578dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a prompt with similar examples in each of the classes\n",
    "def generateDynamicPrompt(input_text, class2TrainDatasetWithEmbedding, N=3):\n",
    "    classwiseSimilarExamples = getClasswiseSimilarExamples(input_text, class2TrainDatasetWithEmbedding)\n",
    "    dynamic_prompt = \"\\nFollowing are some examples.\"\n",
    "    for label in classwiseSimilarExamples:\n",
    "        for example in classwiseSimilarExamples[label][:N]:\n",
    "            dynamic_prompt += \"\\nquestion: \" + example\n",
    "            dynamic_prompt += \"\\noutput: \" + label\n",
    "    return dynamic_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc17023f-678b-4386-b7e7-541282b30f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "predictions = []\n",
    "for example in dataset['test']:\n",
    "    fewshot_examples = generateDynamicPrompt(example[text_key], class2TrainDatasetWithEmbedding, N=1)\n",
    "    dynamic_prompt = task_prompt + fewshot_examples + query_prompt + example[text_key] + answer_prompt\n",
    "    pred = generate_using_gpt3(dynamic_prompt)\n",
    "    pred=trim_text(pred)\n",
    "    labels.append(example[label_type])\n",
    "    predictions.append(class2id[pred])\n",
    "        \n",
    "report = classification_report(labels, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "826584d8-8223-4f88-89cc-65181212a1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      1.00      0.82         9\n",
      "           1       0.61      0.80      0.69        94\n",
      "           2       0.88      0.68      0.77       138\n",
      "           3       0.95      0.88      0.91        65\n",
      "           4       0.93      0.86      0.90        81\n",
      "           5       0.90      0.98      0.94       113\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.83      0.87      0.84       500\n",
      "weighted avg       0.85      0.83      0.83       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
